{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "174cae2b6876a669",
   "metadata": {},
   "source": [
    "# DDQN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1f860602ec5b8a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T07:00:17.322225Z",
     "start_time": "2024-06-26T07:00:14.103036Z"
    }
   },
   "outputs": [],
   "source": [
    "from project.env_system import ManufacturingSystem\n",
    "from gymnasium.wrappers import NormalizeReward\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from scipy.stats import erlang\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Use a double ended queue (deque) for memory\n",
    "# When memory is full, this will replace the oldest value with the new one\n",
    "from collections import deque\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d46617c864c59",
   "metadata": {},
   "source": [
    "#  Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af3d27a8c81b3e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discount rate of future rewards\n",
    "GAMMA = 0.99\n",
    "# Learning rate for NN\n",
    "LEARNING_RATE = 0.003\n",
    "# Tot time step per episode\n",
    "SIM_DURATION= 5000\n",
    "# Training episodes\n",
    "TRAINING_EPISODES= 1_000\n",
    "# Training steps\n",
    "TRAINING_STEPS = TRAINING_EPISODES * SIM_DURATION\n",
    "# Max number of games steps( state,action,reward, next state) \n",
    "MEMORY_SIZE = 10_000_000\n",
    "# Sample batch size for policy Net update \n",
    "BATCH_SIZE = 5\n",
    "# Number of game steps to play before starting training (all random actions)\n",
    "REPLAY_START_SIZE = 50_000\n",
    "# Number of steps between policy -> target net update \n",
    "SYNC_TARGET_STEPS = 1000\n",
    "# Exploration rate (epsilon) is probability of choosing a random action\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.005\n",
    "# Reduction in epsilon \n",
    "EXPLORATION_DECAY = TRAINING_EPISODES * 1000\n",
    "# test episodes\n",
    "TEST_EPISODES = 100\n",
    "# inter val episode we save the current checkpoint  of the training models\n",
    "SAVE_CHECKPOINT = 300 * SIM_DURATION\n",
    "# Save results\n",
    "RESULTS_NAME = 'DDQN'\n",
    "# path for save the training weights\n",
    "check_p_path = 'weights/checkpoints/DDQN'\n",
    "# path for save best net weights\n",
    "best_path = 'weights/best/DDQN'\n",
    "# if we are in a phase of restart the training\n",
    "RESTART_TRAINGING = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# SIM PARAMETERS\n",
    "MAX_WIP= 80\n",
    "ARRIVAL_TIME = 1/5.1\n",
    "RANDOM_SEED= 42\n",
    "EVAL_DAYS= 2500\n",
    "WARMUP_PERIOD= 114\n",
    "WIP_TOLERANCE= 7\n",
    "DAYS_LOOKBACK= 10\n",
    "N_TRACKERS= 33\n",
    "SIM_TIME_STEP = 3\n",
    "MIN_DUE_DATE = 77.00\n",
    "MAX_DUE_DATE = 110\n",
    "REWORK= 0.2\n",
    "VERBOSE= False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd479e59942a0f7",
   "metadata": {},
   "source": [
    "# DQN class \n",
    "#  Used for both policy and target nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2a11efe69b03ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Q Network class\n",
    "    \"\"\"\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        \"\"\"\n",
    "        set up neural nets\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        # neurons per hidden  layer = 2 * observations space => 660\n",
    "        neurons_per_layer = observation_space\n",
    "        \n",
    "        # set strating exploration rate\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        \n",
    "        # set un action space\n",
    "        self.action_space = action_space\n",
    "        self.obs_space = observation_space\n",
    "        \n",
    "        # set up the device for make  calculations\n",
    "        # CPU should be faster for this case wit GPU\n",
    "        self.device = 'cpu'\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(observation_space, neurons_per_layer).to(self.device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(neurons_per_layer, neurons_per_layer).to(self.device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(neurons_per_layer, neurons_per_layer).to(self.device),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(neurons_per_layer, action_space).to(self.device)\n",
    "        ).to(self.device)\n",
    "        \n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Act either randomly or by redicting action that gives max Q\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert to Tensor\n",
    "        # reshape state into 2D array with obs as first 'row'\n",
    "        state = torch.tensor(np.reshape(state, [1, self.obs_space]), dtype=torch.float32)\n",
    "        \n",
    "        # Act randomly if random number < exploration rate\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action = random.randrange(self.action_space)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                # Otherwise get predicted Q values of actions\n",
    "                q_values = self.net(state)\n",
    "                # get index of action with best Q\n",
    "                action = np.argmax(q_values.detach().numpy()[0])\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass and return the action values \"\"\"\n",
    "        x = x.to(self.device)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741908ead816ddb0",
   "metadata": {},
   "source": [
    "#  Policy net training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d510bcb12294073",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimize(policy_net, target_net, memory):\n",
    "    # Do not try to train model if memory is less than required batch size\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    # Sample a random batch from memory\n",
    "    batch = random.sample(memory, BATCH_SIZE)\n",
    "    states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "    # pre processing task\n",
    "    states = torch.Tensor(states)\n",
    "    actions = torch.LongTensor(actions).unsqueeze(-1)  # [bs,] --> [bs, 1]\n",
    "    next_states = torch.Tensor(next_states)\n",
    "    rewards = torch.Tensor(rewards)\n",
    "    terminals = torch.Tensor(terminals)\n",
    "\n",
    "\n",
    "    # Get the Q values for current states\n",
    "    state_action_values = policy_net(states) # [bs,2]\n",
    "\n",
    "    # Get the Q values for next states using the policy net and detach next state values from gradients to prevent updates\n",
    "    policy_next_state_values = policy_net(next_states).detach()\n",
    "\n",
    "    # Get the best actions for the next states\n",
    "    best_actions = torch.argmax(policy_next_state_values, dim=1).unsqueeze(-1) # [bs,] --> [bs, 1]\n",
    "\n",
    "    # Get the Q values for next states using the target net\n",
    "    next_state_action_values = target_net(next_states).detach()\n",
    "    best_next_q_values = next_state_action_values.gather(1, best_actions).squeeze(1)\n",
    "\n",
    "    # Calculate the updated Q values\n",
    "    updated_q_values = rewards + (GAMMA * best_next_q_values * (1 - terminals))\n",
    "\n",
    "    # Get the expected state-action values\n",
    "    expected_state_action_values = state_action_values.clone()\n",
    "    expected_state_action_values[range(BATCH_SIZE), actions] = updated_q_values\n",
    "\n",
    "    # set net to traning mode\n",
    "    policy_net.train()\n",
    "    # reset net gradients\n",
    "    policy_net.optimizer.zero_grad()\n",
    "    # calculate the loss\n",
    "\n",
    "    loss = nn.functional.mse_loss(state_action_values, expected_state_action_values)\n",
    "    # backpropagation loss\n",
    "    loss.backward()\n",
    "    \n",
    "    # clamp the gradient \n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "        \n",
    "    # update net gradients\n",
    "    policy_net.optimizer.step()\n",
    "\n",
    "\n",
    "    return loss.detach().item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8e00e0733b3e66",
   "metadata": {},
   "source": [
    "#   Memory class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c91f32e9e0cd270",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    \"\"\"\n",
    "    Replay memory used to train model.\n",
    "    Limited length memory (using deque, double ended queue from collections).\n",
    "      - When memory full deque replaces oldest data with newest.\n",
    "    Holds, state, action, reward, next state, and episode done.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Store the values\n",
    "        \"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845070ed0176668d",
   "metadata": {},
   "source": [
    "#  Results plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "729e174e10fd52bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(exploration, loss,  returns, wip, throughput, ea_ta_ti, lengths, agent_releases):\n",
    "    \"\"\"\"Plot and report results at end of run\"\"\"\n",
    "    \n",
    "    # Set up chart (ax1 and ax2 share x-axis to combine two plots on one graph)\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    # Plot results\n",
    "    lns1 = ax1.plot(\n",
    "         exploration, label='exploration', color='g', linestyle=':')\n",
    "\n",
    "    lns2 = ax2.plot( wip,\n",
    "             label='WIP', color='r')\n",
    "    \n",
    "    # Get combined legend\n",
    "    lns = lns1 + lns2 \n",
    "    labs = [l.get_label() for l in lns]\n",
    "    ax1.legend(lns, labs, loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=3)\n",
    "    \n",
    "\n",
    "    # Set axes\n",
    "    ax1.set_xlabel('run')\n",
    "    ax1.set_ylabel('exploration')\n",
    "    ax2.set_ylabel('WIP in System')\n",
    "    filename = 'output/' + RESULTS_NAME +'/' + RESULTS_NAME + '_wip.png'\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # chart for throughput-exploration\n",
    "     # Set up chart (ax1 and ax2 share x-axis to combine two plots on one graph)\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    # Plot results\n",
    "    lns1 = ax1.plot(\n",
    "         exploration, label='exploration', color='g', linestyle=':')\n",
    "\n",
    "    lns2 = ax2.plot( throughput,\n",
    "             label='Throughput', color='y')\n",
    "\n",
    "    # Get combined legend\n",
    "    lns = lns1 + lns2\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    ax1.legend(lns, labs, loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=3)\n",
    "\n",
    "\n",
    "    # Set axes\n",
    "    ax1.set_xlabel('run')\n",
    "    ax1.set_ylabel('exploration')\n",
    "    ax2.set_ylabel('Throughput')\n",
    "    plt.title('Monthly throughput Distribution')\n",
    "    filename = 'output/' + RESULTS_NAME +'/' + RESULTS_NAME + '_throughput.png'\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # chart of loss\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "\n",
    "\n",
    "    lns1 = ax1.plot(\n",
    "         loss, label='loss', color='g', linestyle=':')\n",
    "\n",
    "    # Set axes\n",
    "    ax1.set_xlabel('run')\n",
    "    ax1.set_ylabel('loss')\n",
    "    plt.title('Loss Distribution')\n",
    "    filename = 'output/' + RESULTS_NAME +'/' + RESULTS_NAME + '_loss.png'\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # chart of returns\n",
    "    plt.plot(returns)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Episode Return')\n",
    "    filename = 'output/' + RESULTS_NAME +'/' + RESULTS_NAME + '_returns.png'\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "     # chart of lengths\n",
    "    plt.plot(lengths)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Episode Length')\n",
    "    plt.title('Episode Length Distribution')\n",
    "    filename = 'output/' + RESULTS_NAME + '/' + RESULTS_NAME + 'e_length.png'\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # chart Agent releases\n",
    "\n",
    "    plt.plot(agent_releases,\n",
    "             label='Agent releases', color='y')\n",
    "\n",
    "    # Set axes\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Rate of release')\n",
    "    filename = 'output/' + RESULTS_NAME + '/' + RESULTS_NAME + '_agent_releases.png'\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a new figure for plotting the 3d scatter\n",
    "    fig = plt.figure()\n",
    "\n",
    "    # Extracting x, y, z coordinates from the data\n",
    "    x = [point[0] for point in ea_ta_ti]\n",
    "    y = [point[1] for point in ea_ta_ti]\n",
    "    z = [point[2] for point in ea_ta_ti]\n",
    "\n",
    "\n",
    "    # Add a 3D subplot\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Create a scatter plot in 3D\n",
    "    ax.scatter(x, y, z)\n",
    "\n",
    "    # Create a colormap\n",
    "    gradient = np.linspace(0, 1, len(ea_ta_ti))\n",
    "\n",
    "    cool = plt.colormaps['cool']\n",
    "\n",
    "    # Create a scatter plot in 3D with a color gradient\n",
    "    sc = ax.scatter(x, y, z, c=gradient, cmap=cool)\n",
    "\n",
    "    # Highlight the first point\n",
    "    ax.scatter(x[0], y[0], z[0], color='red', s=100, label='First run')\n",
    "    ax.text(x[0], y[0], z[0], 'First', color='red')\n",
    "\n",
    "    # Highlight the last point\n",
    "    ax.scatter(x[-1], y[-1], z[-1], color='green', s=100, label='Last run')\n",
    "    ax.text(x[-1], y[-1], z[-1], 'Last', color='green')\n",
    "\n",
    "\n",
    "\n",
    "    # Customize the tick labels\n",
    "    # Set color gradient legend with only first and last ticks\n",
    "    cbar = plt.colorbar(sc, ax=ax, ticks=[0, 1], orientation='vertical', pad=0.1)\n",
    "    cbar.set_label('Runs')\n",
    "    cbar.ax.set_yticklabels([1, len(ea_ta_ti)])\n",
    "\n",
    "    # Labeling the axes\n",
    "    ax.set_xlabel('EA')\n",
    "    ax.set_ylabel('TA')\n",
    "    ax.set_zlabel('TI')\n",
    "\n",
    "    # Set a title for the plot\n",
    "    ax.set_title('Job dones distribution')\n",
    "    ax.legend()\n",
    "\n",
    "    filename = 'output/' + RESULTS_NAME +'/' + RESULTS_NAME + '_scatter.png'\n",
    "    plt.savefig(filename, dpi=300)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c79e8b74915a62",
   "metadata": {},
   "source": [
    "#   Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83de4399184be348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddqn_company():\n",
    "    \"\"\"Main program loop\"\"\"\n",
    "    ############################################################################\n",
    "    #                          1 Set up Gym+SimPy environment                  #\n",
    "    ############################################################################\n",
    "    sim = ManufacturingSystem(\n",
    "        inter_arrival_time_distribution=lambda: random.expovariate(lambd=ARRIVAL_TIME), \n",
    "        service_time_distribution=lambda x,y: erlang.rvs(5,loc=(x - 5 *(y/5)**(1/2)),scale=(y/5)**(1/2)),\n",
    "        rework_distribution=lambda: random.random() <= REWORK,\n",
    "        due_dates_distribution=lambda : random.uniform(a=MIN_DUE_DATE, b=MAX_DUE_DATE),\n",
    "        warmup_period=WARMUP_PERIOD,\n",
    "        max_wip=MAX_WIP,\n",
    "        sim_duration=SIM_DURATION,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        eval_days=EVAL_DAYS,\n",
    "        wip_tol=WIP_TOLERANCE,\n",
    "        days_lookback=DAYS_LOOKBACK,\n",
    "        n_trackers=N_TRACKERS,\n",
    "        verbose=VERBOSE,\n",
    "        sim_time_step=SIM_TIME_STEP\n",
    "    )\n",
    "    \n",
    "    # normalize the rewards \n",
    "    sim = NormalizeReward(sim)\n",
    "    \n",
    "    # get number of obs returned for state\n",
    "    obs_space = sim.observation_size\n",
    "    \n",
    "    # number of actions possible ( boolean)\n",
    "    action_space = sim.action_size\n",
    "    \n",
    "    ############################################################################\n",
    "    #                    2 Set up policy and target nets                       #\n",
    "    ############################################################################\n",
    "    \n",
    "    # set up and keep best net performance\n",
    "    policy_net = DQN(obs_space, action_space)\n",
    "    target_net = DQN(obs_space, action_space)\n",
    "    best_net = DQN(obs_space, action_space)\n",
    "    \n",
    "    # Set loss function and optimizer\n",
    "    policy_net.optimizer = optim.Adam(\n",
    "        params=policy_net.parameters(), lr=LEARNING_RATE\n",
    "    )\n",
    "    \n",
    "    # copy weights from policy net to target\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    # Set target net to eval for not training it \n",
    "    target_net.eval()\n",
    "    \n",
    "    ############################################################################\n",
    "    #                           3 Set up memory                                #\n",
    "    ############################################################################\n",
    "    \n",
    "    memory = Memory()\n",
    "    \n",
    "    ############################################################################\n",
    "    #                     4 Set up + start training loop                      #\n",
    "    ############################################################################\n",
    "    \n",
    "    # set up run counter and learning loop\n",
    "    all_steps = 0\n",
    "    continue_learning = True\n",
    "    best_reward = -np.inf\n",
    "    \n",
    "    # check if we've to restart the training\n",
    "    if RESTART_TRAINGING:\n",
    "        # load the state dicts and run counter\n",
    "        checkpoint = torch.load(check_p_path)\n",
    "        policy_net.load_state_dict(checkpoint['policy_state_dict'])\n",
    "        target_net.load_state_dict(checkpoint['target_state_dict'])\n",
    "        best_net.load_state_dict(checkpoint['best_state_dict'])\n",
    "        policy_net.optimizer.load_state_dict(checkpoint['policy_opt_state_dict'])\n",
    "        all_steps = checkpoint['epoch']\n",
    "    \n",
    "    # set up list for results\n",
    "    results_losses = []\n",
    "    results_exploration = []\n",
    "    results_returns = []\n",
    "    results_mean_wip = []\n",
    "    results_mean_throughput = []\n",
    "    results_ea_ta_ti = []\n",
    "    results_length = []\n",
    "    results_agent_releases = []\n",
    "    results_psp_length = []\n",
    "    results_agent_not_decide = []\n",
    "    \n",
    "    # Continue repeating episodes until target complete\n",
    "    while continue_learning:\n",
    "        # play episode\n",
    "\n",
    "        \n",
    "        # reset env \n",
    "        state = sim.reset()\n",
    "        \n",
    "        # reset lists ( we remember the last all steps counter for compute the length of the episode)\n",
    "        prev_all_steps = all_steps\n",
    "        all_steps += WARMUP_PERIOD\n",
    "        e_loss = 0\n",
    "        tot_reward = 0\n",
    "        rewards = []\n",
    "        \n",
    "\n",
    "        \n",
    "        # continue loop until episode complete or truncated\n",
    "        while True:\n",
    "\n",
    "            # get action to take ( set eval mode to avoid dropout layers)\n",
    "            policy_net.eval()\n",
    "            action = policy_net.act(state)\n",
    "            \n",
    "            # play action ( get S', R, T, TR)\n",
    "    \n",
    "            # Act \n",
    "            state_next, reward, terminal, truncated, info = sim.step(action)\n",
    "            \n",
    "            tot_reward += reward\n",
    "            \n",
    "            # update trackers \n",
    "            all_steps += SIM_TIME_STEP\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            \n",
    "             \n",
    "            if not truncated:\n",
    "                # record state, action, reward new state & terminal \n",
    "                memory.remember(state, action, reward, state_next, terminal)\n",
    "            \n",
    "            # update state \n",
    "            state = state_next\n",
    "            \n",
    "            # check if end of episode\n",
    "            if terminal and not truncated:\n",
    "                # Clear print row content \n",
    "                clear_row = '\\r' + ' ' * 500 + '\\r'\n",
    "                print(clear_row, end='')\n",
    "                process = (all_steps / TRAINING_STEPS) * 100\n",
    "                # we block when we reach the finish\n",
    "                process = min(100, process)\n",
    "                print(f'Training Progress: {process:4.1f}%, ', end='')\n",
    "                length = all_steps - prev_all_steps\n",
    "                print(f'Episode length: {length:4.1f}, ', end='')\n",
    "                # get expo rate\n",
    "                exploration = policy_net.exploration_rate\n",
    "                print(f'Exploration: {exploration: .3f}, ', end='')    \n",
    "                avg_wip = np.mean(info['wip in system'])\n",
    "                print(f'Average Tot WIP: {avg_wip:4.1f}, ', end='')\n",
    "                throughput = 30 *(info['number of job dones'] / SIM_DURATION)\n",
    "                print(f'Throughput: {throughput:4.1f}, ', end='')\n",
    "                last_ea_ta_ti = info['EA_TA_TI'][-1]\n",
    "                # trasform in np array\n",
    "                last_ea_ta_ti = np.array(last_ea_ta_ti)\n",
    "                last_ea_ta_ti = (last_ea_ta_ti /info['number of job dones']) * 100\n",
    "                print(f'EA: {last_ea_ta_ti[0]:4.1f}%, ', end='')\n",
    "                print(f'TA: {last_ea_ta_ti[1]:4.1f}%, ', end='')\n",
    "                print(f'TI: {last_ea_ta_ti[2]:4.1f}%, ', end='')\n",
    "                jobs_create = info['number of job create']\n",
    "                print(f'jobs create: {jobs_create}, ', end='')\n",
    "                rate_agent_release = info['Agent releases']\n",
    "                print(f'Rate of agent release: {rate_agent_release:4.1f}%, ', end='')\n",
    "                rate_agent_not_decide = info['Agent not decide']\n",
    "                print(f'Rate of agent not decide: {rate_agent_not_decide:4.1f}%, ', end='')\n",
    "                psp_length = info['psp_list']\n",
    "                print(f'PSP Length: {psp_length:4.1f} ', end='')\n",
    "\n",
    "                # add to  results lists\n",
    "                results_length.append(length)\n",
    "                results_losses.append(e_loss/SIM_DURATION)\n",
    "                results_exploration.append(exploration)\n",
    "                results_returns.append(tot_reward)\n",
    "                results_mean_wip.append(avg_wip)\n",
    "                results_mean_throughput.append(throughput)\n",
    "                results_ea_ta_ti.append(last_ea_ta_ti)\n",
    "                results_agent_releases.append(rate_agent_release)\n",
    "                results_psp_length.append(psp_length)\n",
    "                results_agent_not_decide.append(rate_agent_not_decide)\n",
    "                \n",
    "                # Save model if best reward\n",
    "                if tot_reward > best_reward:\n",
    "                    best_reward = tot_reward\n",
    "                    # copy weights \n",
    "                    best_net.load_state_dict(policy_net.state_dict())\n",
    "                \n",
    "                # check for end  of learning \n",
    "                if all_steps >= TRAINING_STEPS:\n",
    "                    continue_learning = False\n",
    "                \n",
    "                # end episode loop\n",
    "                break\n",
    "            \n",
    "            # check if truncated episode\n",
    "            if truncated:\n",
    "                \n",
    "                # Clear print row content \n",
    "                clear_row = '\\r' + ' ' * 500 + '\\r'\n",
    "                print(clear_row, end='')\n",
    "                process = (all_steps / TRAINING_STEPS) * 100\n",
    "                # we block the max\n",
    "                process = min(100, process)\n",
    "                print(f'Training Progress: {process:4.1f}%, ', end='')\n",
    "                length = all_steps - prev_all_steps\n",
    "                print(f'Episode length: {length:4.1f}, ', end='')\n",
    "                # get expo rate\n",
    "                exploration = policy_net.exploration_rate\n",
    "                print(f'Exploration: {exploration: .3f}, ', end='')\n",
    "                avg_wip = np.mean(info['wip in system'])\n",
    "                print(f'Average Tot WIP: {avg_wip:4.1f}, ', end='')\n",
    "                throughput = 30 *(info['number of job dones'] / SIM_DURATION)\n",
    "                print(f'Throughput: {throughput:4.1f}, ', end='')\n",
    "                last_ea_ta_ti = info['EA_TA_TI'][-1]\n",
    "                # trasform in np array\n",
    "                last_ea_ta_ti = np.array(last_ea_ta_ti)\n",
    "                last_ea_ta_ti = (last_ea_ta_ti /info['number of job dones']) * 100\n",
    "                print(f'EA: {last_ea_ta_ti[0]:4.1f}%, ', end='')\n",
    "                print(f'TA: {last_ea_ta_ti[1]:4.1f}%, ', end='')\n",
    "                print(f'TI: {last_ea_ta_ti[2]:4.1f}%, ', end='')\n",
    "                jobs_create = info['number of job create']\n",
    "                print(f'jobs create: {jobs_create}, ', end='')\n",
    "                rate_agent_release = info['Agent releases']\n",
    "                print(f'Rate of agent release: {rate_agent_release:4.1f}%, ', end='')\n",
    "                rate_agent_not_decide = info['Agent not decide']\n",
    "                print(f'Rate of agent not decide: {rate_agent_not_decide:4.1f}%, ', end='')\n",
    "                psp_length = info['psp_list']\n",
    "                print(f'PSP Length: {psp_length:4.1f} ', end='')\n",
    "                \n",
    "                # add to  results lists\n",
    "                results_length.append(length)\n",
    "                results_exploration.append(exploration)\n",
    "                results_losses.append(e_loss/SIM_DURATION)\n",
    "                results_returns.append(tot_reward)\n",
    "                results_mean_wip.append(avg_wip)\n",
    "                results_mean_throughput.append(throughput)\n",
    "                results_ea_ta_ti.append(last_ea_ta_ti)\n",
    "                results_agent_releases.append(rate_agent_release)\n",
    "                results_psp_length.append(psp_length)\n",
    "                results_agent_not_decide.append(rate_agent_not_decide)\n",
    "\n",
    "                # check for end  of learning\n",
    "                if all_steps >= TRAINING_STEPS:\n",
    "                    continue_learning = False\n",
    "\n",
    "                # end episode loop\n",
    "                break\n",
    "\n",
    "            # update policy net \n",
    "            \n",
    "            # avoid training model if memory is not sufficient \n",
    "            if len(memory.memory) > REPLAY_START_SIZE:\n",
    "\n",
    "                # Reduce exploration rate\n",
    "                policy_net.exploration_rate = EXPLORATION_MIN + (EXPLORATION_MAX - EXPLORATION_MIN) * np.exp(\n",
    "                        -1. * all_steps / EXPLORATION_DECAY)\n",
    "\n",
    "                # update policy net\n",
    "                loss = optimize(policy_net, target_net, memory.memory)\n",
    "                \n",
    "                if loss is not None:\n",
    "                    e_loss += loss\n",
    "                # update target net periodically\n",
    "                # Use load_state_dict method to copy weights from policy net\n",
    "                if all_steps % SYNC_TARGET_STEPS == 0:\n",
    "                    target_net.load_state_dict(policy_net.state_dict())\n",
    "                \n",
    "                    \n",
    "            # check if we've to save the model\n",
    "            if all_steps % SAVE_CHECKPOINT == 0:\n",
    "                torch.save({\n",
    "                    'epoch': all_steps,\n",
    "                    'policy_state_dict': policy_net.state_dict(),\n",
    "                    'target_state_dict': target_net.state_dict(),\n",
    "                    'best_state_dict': best_net.state_dict(),\n",
    "                    'policy_opt_state_dict': policy_net.optimizer.state_dict(),\n",
    "                }, check_p_path)\n",
    "                # and save results \n",
    "                run_details = pd.DataFrame()\n",
    "                run_details['Episode Length'] = results_length\n",
    "                run_details['loss'] = results_losses\n",
    "                run_details['exploration'] = results_exploration\n",
    "                run_details['Returns'] = results_returns\n",
    "                run_details['mean wip'] = results_mean_wip\n",
    "                run_details['monthly mean throughput'] = results_mean_throughput\n",
    "                run_details['Agent releases'] = results_agent_releases\n",
    "                run_details['PSP length'] = results_psp_length\n",
    "                \n",
    "                # convert in np array\n",
    "                ea_ta_ti = np.array(results_ea_ta_ti)\n",
    "                run_details['EA %'] = ea_ta_ti[:, 0]\n",
    "                run_details['TA %'] = ea_ta_ti[:, 1]\n",
    "                run_details['TI %'] = ea_ta_ti[:, 2]\n",
    "                \n",
    "                filename = 'output/' + RESULTS_NAME +'/' + RESULTS_NAME + '_train_result.csv'\n",
    "                run_details.to_csv(filename, index=True, index_label='run')\n",
    "                \n",
    "                \n",
    "    \n",
    "    \n",
    "    ############################################################################\n",
    "    #             5 Learning complete - plot and save results                 #\n",
    "    ############################################################################\n",
    "    \n",
    "    plot_results(results_exploration,results_losses, results_returns, results_mean_wip, results_mean_throughput,results_ea_ta_ti, results_length, results_agent_releases)\n",
    "    \n",
    "    # save results \n",
    "    run_details = pd.DataFrame()\n",
    "    run_details['Episode Length'] = results_length\n",
    "    run_details['loss'] = results_losses\n",
    "    run_details['exploration'] = results_exploration\n",
    "    run_details['Returns'] = results_returns\n",
    "    run_details['mean wip'] = results_mean_wip\n",
    "    run_details['monthly mean throughput'] = results_mean_throughput\n",
    "    run_details['Agent releases'] = results_agent_releases\n",
    "    run_details['Agent not decide'] = results_agent_not_decide\n",
    "    run_details['PSP length'] = results_psp_length\n",
    "    \n",
    "    # convert in np array\n",
    "    ea_ta_ti = np.array(results_ea_ta_ti)\n",
    "    run_details['EA %'] = ea_ta_ti[:, 0]\n",
    "    run_details['TA %'] = ea_ta_ti[:, 1]\n",
    "    run_details['TI %'] = ea_ta_ti[:, 2]\n",
    "    \n",
    "    filename = 'output/' + RESULTS_NAME +'/' + RESULTS_NAME + '_train_result.csv'\n",
    "    run_details.to_csv(filename, index=True, index_label='run')\n",
    "    \n",
    "    # save best net weights\n",
    "    torch.save({\n",
    "        'best_state_dict': best_net.state_dict()\n",
    "    },best_path)\n",
    "    \n",
    "    ############################################################################\n",
    "    #                             Test best model                              #\n",
    "    ############################################################################\n",
    "    print()\n",
    "    print('Test Model')\n",
    "    print('----------')\n",
    "    \n",
    "    best_net.exploration_rate = 0\n",
    "\n",
    "    best_net.eval()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # set results dict \n",
    "    results = dict()\n",
    "    results['wip'] = []\n",
    "    results['monthly throughput'] = []\n",
    "    results['EA'] = []\n",
    "    results['TA'] = []\n",
    "    results['TI'] = []\n",
    "    results['Agent releases'] = []\n",
    "    results['PSP length'] = []\n",
    "    \n",
    "    # Replicate model runs\n",
    "    for run in range(TEST_EPISODES):\n",
    "        # reset env \n",
    "        state = sim.reset()\n",
    "        \n",
    "        # continue loop until episode complete\n",
    "        while True:\n",
    "            # get action to take \n",
    "            best_net.eval()\n",
    "            action = best_net.act(state)\n",
    "            \n",
    "            # Act\n",
    "            state_next, reward, terminal, truncated, info = sim.step(action)\n",
    "            \n",
    "            # update state\n",
    "            state = state_next\n",
    "            \n",
    "            if terminal:\n",
    "                print(f'Run: {run}, ', end='')\n",
    "                avg_wip = np.mean(info['wip in system'])\n",
    "                print(f'Average Tot WIP: {avg_wip:4.1f}, ', end='')\n",
    "                throughput = 30 *(info['number of job dones'] / SIM_DURATION)\n",
    "                print(f'Throughput: {throughput:4.1f}, ', end='')\n",
    "                last_ea_ta_ti = info['EA_TA_TI'][-1]\n",
    "                last_ea_ta_ti = np.array(last_ea_ta_ti)\n",
    "                last_ea_ta_ti = (last_ea_ta_ti /info['number of job dones']) * 100\n",
    "                print(f'EA: {last_ea_ta_ti[0]:4.1f}%, ', end='')\n",
    "                print(f'TA: {last_ea_ta_ti[1]:4.1f}%, ', end='')\n",
    "                print(f'TI: {last_ea_ta_ti[2]:4.1f}%,  ', end='')\n",
    "                rate_agent_release = info['Agent releases']\n",
    "                print(f'Rate of agent release: {rate_agent_release:4.1f}% ', end='')\n",
    "                psp_length = info['psp_list']\n",
    "                print(f'PSP Length: {psp_length:4.1f} ', end='')\n",
    "\n",
    "                print()\n",
    "                # add to results\n",
    "                results['wip'].append(avg_wip)\n",
    "                results['monthly throughput'].append(throughput)\n",
    "                results['EA'].append(last_ea_ta_ti[0])\n",
    "                results['TA'].append(last_ea_ta_ti[1])\n",
    "                results['TI'].append(last_ea_ta_ti[2])\n",
    "                results['Agent releases'].append(rate_agent_release)\n",
    "                results['PSP length'].append(psp_length)\n",
    "                \n",
    "                # End episode loop\n",
    "                break\n",
    "            \n",
    "    \n",
    "    results = pd.DataFrame(results)\n",
    "    filename = 'output/' + RESULTS_NAME +'/' + RESULTS_NAME + '_test_result.csv'\n",
    "    results.to_csv(filename, index=True, index_label='run')\n",
    "    print()\n",
    "    print(results.describe())\n",
    "    return run_details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7f9638ce845640",
   "metadata": {},
   "source": [
    "#  Model Entry Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c156508d5f955537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress: 29.2%, Episode length: 5001.0, Exploration:  0.236, Average Tot WIP: 34.8, Throughput:  4.8, EA:  3.6%, TA: 96.3%, TI:  0.1%, jobs create: 974, Rate of agent release: 48.4%, Rate of agent not decide:  0.4%, PSP Length: 164.0                                                                                                                                                                                                                                                                 "
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    last_run = ddqn_company()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
